{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d457d8ac-2c6a-4b34-9642-cc5302b84999",
   "metadata": {},
   "source": [
    "# Model\n",
    "Random Forest classifier with **100** trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe87a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "070227f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'final_dataset.csv'\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a3248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a703bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"url\",'benign'])\n",
    "y = df['benign']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f22c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74f110a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100,random_state=44)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297f8a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ec057",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce086a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(\"accuracy: \",metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37082cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57e3a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.Series(model.feature_importances_,index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e13391e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "number_rate_after_path    0.000000\n",
       "at_count                  0.000106\n",
       "double_slash_count        0.001783\n",
       "tld_count                 0.006141\n",
       "param_digit_count         0.011432\n",
       "params_path_ratio         0.016989\n",
       "params_url_ratio          0.017718\n",
       "domain_digit_count        0.019298\n",
       "params_domain_ratio       0.019397\n",
       "number_rate_file_name     0.031697\n",
       "number_rate_url           0.033988\n",
       "symbols_count             0.036631\n",
       "percent_count             0.036841\n",
       "hyphen_count              0.047155\n",
       "entropy                   0.059853\n",
       "dot_count                 0.078150\n",
       "avg_path_token_len        0.082618\n",
       "char_cont_rate            0.088813\n",
       "domain_url_ratio          0.100364\n",
       "path_url_ratio            0.141084\n",
       "path_domain_ratio         0.169941\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imp.sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f54a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "from __future__ import division\n",
    "from collections import Counter\n",
    "from tld import get_tld\n",
    "def urlparse(address):\n",
    "    if not re.search(r'^[A-Za-z0-9+.\\-]+://', address):\n",
    "        address = 'http://{0}'.format(address)\n",
    "    return urllib.parse.urlparse(address)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f438c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class URLPreprocessor:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        url = url.replace(r\"'\",'')\n",
    "        self.url = url\n",
    "#         self.benign = benign\n",
    "        self.parsedurl = urlparse(url)\n",
    "        self.tld = get_tld(url, fail_silently=True,fix_protocol=True) \n",
    "        self.domain = self.parsedurl.netloc\n",
    "        self.protocol = self.parsedurl.scheme\n",
    "        self.path = self.parsedurl.path\n",
    "        self.parameters = self.parsedurl.query\n",
    "        self.anchor = self.parsedurl.fragment\n",
    "\n",
    "        self.path_url_ratio = len(self.path) / len(self.url)\n",
    "        self.params_domain_ratio = len(self.parameters) / len(self.domain)\n",
    "        self.params_url_ratio = len(self.parameters) / len(self.url)  \n",
    "        self.domain_url_ratio = len(self.domain) / len(self.url)\n",
    "        self.number_rate_url = len(re.sub(\"[^0-9]\", \"\", self.url)) / len(self.url)\n",
    "        self.path_domain_ratio = len(self.path) / len(self.domain)\n",
    "        self.avg_path_token_len = sum([len(i) for i in self.path.split(\"/\")]) / len(self.path.split(\"/\"))\n",
    "        \n",
    "        #simboloebis counts vamateb imitoro magasac aqvs mnishvneloba turme\n",
    "        # @, //, ., % da - s counts vamateb da kide vamateb zogadad simboloebis counts\n",
    "        self.at_count = url.count(r'@')\n",
    "        self.double_slash_count = url.count(r'//')\n",
    "        self.hyphen_count = url.count(r'-')\n",
    "        self.dot_count = url.count(r'.')\n",
    "        self.percent_count = url.count(r'%')\n",
    "        self.symbols_count = len(re.findall(r'[:/=?.,;()]+',url))\n",
    "        #vamateb top level domainebis counts\n",
    "        try:\n",
    "            self.tld_count = len(self.tld.split('.'))\n",
    "        except:\n",
    "            self.tld_count = 0\n",
    "        #vamateb parametrebshi/queryshi da domainshi digit counts\n",
    "        self.param_digit_count = len(re.findall(r'\\d', self.parameters))\n",
    "        self.domain_digit_count = len(re.findall(r'\\d', self.domain))\n",
    "        \n",
    "        try:\n",
    "            self.number_rate_after_path = len(re.sub(\"[^0-9]\", \"\", self.url.split(\"/\")[-1])) / self.url.split(\"/\")[-1]\n",
    "        except: \n",
    "            self.number_rate_after_path = 0\n",
    "        \n",
    "        try:\n",
    "            self.params_path_ratio = len(self.parameters) / len(self.path)\n",
    "        except:\n",
    "            self.params_path_ratio = 0\n",
    "\n",
    "        try:\n",
    "            self.number_rate_file_name = len(re.sub(\"[^0-9]\", \"\", self.path)) / len(self.path)\n",
    "        except: \n",
    "            self.number_rate_file_name = 0\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def char_cont_rate(self):\n",
    "        symbol_sequences = re.findall(r'[^a-zA-Z0-9]', self.domain)\n",
    "        total_len = len(self.domain)\n",
    "    \n",
    "        current_char_sequence = 0\n",
    "        longest_char_sequence = 0\n",
    "\n",
    "        current_digit_sequence = 0\n",
    "        longest_digit_sequence = 0\n",
    "        \n",
    "        current_sym_sequence = 0\n",
    "        longest_sym_sequence = 0\n",
    "\n",
    "        for char in self.domain:\n",
    "            if char.isalpha():\n",
    "                current_char_sequence += 1\n",
    "                current_digit_sequence = 0  # Reset digit sequence count\n",
    "            elif char.isdigit():\n",
    "                current_digit_sequence += 1\n",
    "                current_char_sequence = 0  # Reset character sequence count\n",
    "            else:\n",
    "                current_char_sequence = 0\n",
    "                current_digit_sequence = 0\n",
    "                \n",
    "            if char in symbol_sequences:\n",
    "                current_sym_sequence +=1\n",
    "            else:\n",
    "                longest_sym_sequence = max(longest_sym_sequence,current_sym_sequence)\n",
    "                current_sym_sequence = 0\n",
    "            \n",
    "            longest_char_sequence = max(longest_char_sequence, current_char_sequence)\n",
    "            longest_digit_sequence = max(longest_digit_sequence, current_digit_sequence)\n",
    "            \n",
    "        return sum([longest_char_sequence,longest_digit_sequence,longest_sym_sequence])/total_len        \n",
    "    \n",
    "\n",
    "    def entropy(self):\n",
    "        \n",
    "        base = 2\n",
    "        data = self.domain\n",
    "        entropy = 0.0\n",
    "        #remove dots and tld\n",
    "        try:\n",
    "            data = data.replace(self.tld,'')\n",
    "        except:\n",
    "            pass\n",
    "        data = data.replace('.','').replace(\":\",'').replace(r'%','')\n",
    "        data = data.lower()\n",
    "        length = len(data) * 1.0\n",
    "\n",
    "        if length > 0:\n",
    "            cnt = Counter(data)\n",
    "            # These probability numbers were calculated from the Alexa Top\n",
    "            # 1 million domains as of September 15th, 2017. TLDs and instances\n",
    "            # of 'www' were removed so 'www.google.com' would be treated as\n",
    "            # 'google' and 'images.google.com' would be 'images.google'.\n",
    "            probabilities = {\n",
    "                '-': 0.013342298553905901,\n",
    "                #arvici @ simbolos probability da yvelaze dabals mivanicheb imitoro saertod arunda iyos\n",
    "                '@': 9.04562613824129e-08,\n",
    "                '_': 9.04562613824129e-06,\n",
    "                '0': 0.0024875471880163543,\n",
    "                '1': 0.004884638114650296,\n",
    "                '2': 0.004373560237839663,\n",
    "                '3': 0.0021136613076357144,\n",
    "                '4': 0.001625197496170685,\n",
    "                '5': 0.0013070929769758662,\n",
    "                '6': 0.0014880054997406921,\n",
    "                '7': 0.001471421851820583,\n",
    "                '8': 0.0012663876593537805,\n",
    "                '9': 0.0010327089841158806,\n",
    "                'a': 0.07333590631143488,\n",
    "                'b': 0.04293204925644953,\n",
    "                'c': 0.027385633133525503,\n",
    "                'd': 0.02769469202658208,\n",
    "                'e': 0.07086192756262588,\n",
    "                'f': 0.01249653250998034,\n",
    "                'g': 0.038516276096631406,\n",
    "                'h': 0.024017645001386995,\n",
    "                'i': 0.060447396668797414,\n",
    "                'j': 0.007082725266242929,\n",
    "                'k': 0.01659570875496002,\n",
    "                'l': 0.05815885325582237,\n",
    "                'm': 0.033884915513851865,\n",
    "                'n': 0.04753175014774523,\n",
    "                'o': 0.09413783122067709,\n",
    "                'p': 0.042555148167356144,\n",
    "                'q': 0.0017231917793349655,\n",
    "                'r': 0.06460084667060655,\n",
    "                's': 0.07214640647425614,\n",
    "                't': 0.06447722311338391,\n",
    "                'u': 0.034792493336388744,\n",
    "                'v': 0.011637198026847418,\n",
    "                'w': 0.013318176884203925,\n",
    "                'x': 0.003170491961453572,\n",
    "                'y': 0.016381628936354975,\n",
    "                'z': 0.004715786426736459\n",
    "            }\n",
    "\n",
    "            for char, count in cnt.items():\n",
    "                observed = count / length\n",
    "                expected = probabilities[char]\n",
    "                entropy += observed * math.log((observed / expected), base)\n",
    "        return entropy\n",
    "\n",
    "        \n",
    "\n",
    "    def process(self):\n",
    "        entropy = self.entropy()\n",
    "        char_cont_rate = self.char_cont_rate()\n",
    "        return {\n",
    "            \"url\":self.url,\n",
    "#             \"benign\":self.benign,\n",
    "            \"number_rate_file_name\" : self.number_rate_file_name,\n",
    "            \"domain_url_ratio\" : self.domain_url_ratio,\n",
    "            \"number_rate_url\" : self.number_rate_url,\n",
    "            \"path_domain_ratio\" : self.path_domain_ratio,\n",
    "            \"number_rate_after_path\" : self.number_rate_after_path,\n",
    "            \"avg_path_token_len\" : self.avg_path_token_len,\n",
    "            \"params_path_ratio\" : self.params_path_ratio,\n",
    "            \"params_url_ratio\" : self.params_url_ratio,\n",
    "            \"params_domain_ratio\" : self.params_domain_ratio,\n",
    "            \"path_url_ratio\" : self.path_url_ratio,\n",
    "            \"entropy\" : entropy,\n",
    "            \"char_cont_rate\" : char_cont_rate,\n",
    "            \"at_count\":self.at_count,\n",
    "            \"double_slash_count\":self.double_slash_count,\n",
    "            \"hyphen_count\":self.hyphen_count,\n",
    "            \"dot_count\":self.dot_count,\n",
    "            \"symbols_count\":self.symbols_count,\n",
    "            \"tld_count\":self.tld_count,\n",
    "            \"domain_digit_count\":self.domain_digit_count,\n",
    "            \"param_digit_count\":self.param_digit_count,\n",
    "            \"percent_count\":self.percent_count\n",
    "            \n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b4f71d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(url_list):\n",
    "    url_in = []\n",
    "    for url in url_list:\n",
    "        url_processor = URLPreprocessor(url)\n",
    "        url_in.append(list(url_processor.process().values()))\n",
    "    output = model.predict(url_in)\n",
    "    output = [str(prediction) for prediction in output]\n",
    "\n",
    "    return {\"url\":output[0]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98a3dc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'https://www.youtube.com'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://www.youtube.com\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 6\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(url_list)\u001b[0m\n\u001b[0;32m      4\u001b[0m     url_processor \u001b[38;5;241m=\u001b[39m URLPreprocessor(url)\n\u001b[0;32m      5\u001b[0m     url_in\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlist\u001b[39m(url_processor\u001b[38;5;241m.\u001b[39mprocess()\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m----> 6\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m output \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(prediction) \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m output]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m:output[\u001b[38;5;241m0\u001b[39m]}\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:823\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    803\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 823\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    826\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:865\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    863\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    868\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:599\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    598\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 599\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\sklearn\\base.py:605\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    603\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 605\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    607\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\sklearn\\utils\\validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    913\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 915\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    918\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    919\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'https://www.youtube.com'"
     ]
    }
   ],
   "source": [
    "pipeline([\"https://www.youtube.com\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d97fa90b",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xad in position 131194: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./urlset.csv/urlset.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xad in position 131194: invalid start byte"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./urlset.csv/urlset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee9be8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac70f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
